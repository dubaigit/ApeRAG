image:
  repository: "docker.io/apecloud/aperag"  # Full image name including registry
  tag: "v0.0.0-nightly"
  pullPolicy: IfNotPresent

nameOverride: ""
fullnameOverride: ""

podAnnotations: {}

service:
  type: ClusterIP

ingress:
  className: ""
  annotations: {}

postgres:
  enabled: true  # Enable/disable PostgreSQL database integration
  POSTGRES_HOST: "pg-cluster-postgresql-postgresql"
  POSTGRES_PORT: "5432"
  POSTGRES_DB: "postgres"
  # The name of the Secret containing PostgreSQL username and password (e.g., "username" and "password" keys).
  # Example: "pg-cluster-postgresql-account-postgres" (references your Kubeblocks credential Secret)
  POSTGRES_CREDENTIALS_SECRET_NAME: "pg-cluster-postgresql-account-postgres"
  # Username and Password Priority:
  # 1. If POSTGRES_CREDENTIALS_SECRET_NAME is set, attempts to fetch 'username' and 'password' from that Secret.
  # 2. If POSTGRES_CREDENTIALS_SECRET_NAME is empty, uses the direct values of POSTGRES_USER and POSTGRES_PASSWORD.
  # 3. If direct values are also empty, defaults to "postgres" for user or forces a failure for password.
  # ⚠️ Recommend to use POSTGRES_CREDENTIALS_SECRET_NAME
  POSTGRES_USER: "postgres" # Default PostgreSQL user
  # Password (Sensitive information, strongly recommended NOT to set directly in values.yaml):
  # Only use this if you are NOT using POSTGRES_CREDENTIALS_SECRET_NAME and accept the security implications.
  # ⚠️ Strongly discourage directly setting sensitive passwords here. Use Secrets for production.
  # ⚠️ Recommend to use POSTGRES_CREDENTIALS_SECRET_NAME
  POSTGRES_PASSWORD: ""

redis:
  enabled: true  # Enable/disable Redis database integration
  REDIS_HOST: "redis-cluster-redis-redis"
  REDIS_PORT: "6379"
  # The name of the Secret containing Redis credentials (e.g., "password" key).
  # Example: "redis-cluster-redis-account-default"
  REDIS_CREDENTIALS_SECRET_NAME: "redis-cluster-redis-account-default" # Or your actual Redis secret name
  # User for Redis connection. Default: "default"
  # ⚠️ Recommend to use REDIS_CREDENTIALS_SECRET_NAME
  REDIS_USER: "default"
  # Password for Redis connection.
  # ⚠️ Strongly discourage directly setting sensitive passwords here. Use Secrets for production.
  # ⚠️ Recommend to use REDIS_CREDENTIALS_SECRET_NAME
  REDIS_PASSWORD: "redis"

elasticsearch:
  enabled: true  # Enable/disable Elasticsearch integration (used for hybrid search)
  # Hostname for the Elasticsearch HTTP service.
  # Default: "es-cluster-mdit-http" (Kubeblocks Elasticsearch SVC name)
  ES_HOST: "es-cluster-mdit-http"
  # Port for the Elasticsearch HTTP service. Default: "9200"
  ES_PORT: "9200"
  # Protocol for Elasticsearch connection. Default: "http" or "https"
  ES_PROTOCOL: "http"
  # The name of the Secret containing Elasticsearch credentials (e.g., "username" and "password" keys).
  # Example: "es-cluster-mdit-account-default"
  ES_CREDENTIALS_SECRET_NAME: "" # Fill this if you have an ES secret
  # User for Elasticsearch connection (if authentication is enabled).
  # ⚠️ Recommend to use ES_CREDENTIALS_SECRET_NAME
  ES_USER: ""
  # Password for Elasticsearch connection (if authentication is enabled).
  # ⚠️ Strongly discourage directly setting sensitive passwords here. Use Secrets for production.
  # ⚠️ Recommend to use ES_CREDENTIALS_SECRET_NAME
  ES_PASSWORD: ""

neo4j:
  enabled: false  # Enable/disable Neo4j graph database integration (optional for graph features)
  # Hostname for the Neo4j Bolt service.
  # Default: "neo4j-cluster-neo4j" (Kubeblocks Neo4j SVC name)
  NEO4J_HOST: "neo4j-cluster-neo4j"
  # Port for the Neo4j Bolt service. Default: "7687"
  NEO4J_PORT: "7687"
  # The name of the Secret containing Neo4j username and password (e.g., "username" and "password" keys).
  # Example: "neo4j-cluster-neo4j-account-neo4j" (references your Kubeblocks credential Secret)
  NEO4J_CREDENTIALS_SECRET_NAME: "neo4j-cluster-neo4j-account-neo4j"
  # Username and Password Priority:
  # 1. If NEO4J_CREDENTIALS_SECRET_NAME is set, attempts to fetch 'username' and 'password' from that Secret.
  # 2. If NEO4J_CREDENTIALS_SECRET_NAME is empty, uses the direct values of NEO4J_USERNAME and NEO4J_PASSWORD.
  # 3. If direct values are also empty, defaults to "neo4j" for user or forces a failure for password.
  # ⚠️ Recommend to use NEO4J_CREDENTIALS_SECRET_NAME
  NEO4J_USERNAME: "neo4j" # Default Neo4j user
  # Password (Sensitive information, strongly recommended NOT to set directly in values.yaml):
  # Only use this if you are NOT using NEO4J_CREDENTIALS_SECRET_NAME and accept the security implications.
  # ⚠️ Strongly discourage directly setting sensitive passwords here. Use Secrets for production.
  # ⚠️ Recommend to use NEO4J_CREDENTIALS_SECRET_NAME
  NEO4J_PASSWORD: ""

nebula:
  enabled: false  # Enable/disable NebulaGraph database integration (optional for graph features)
  # Hostname for the NebulaGraph Graph service.
  # Default: "nebula-cluster-graphd" (Kubeblocks NebulaGraph SVC name)
  NEBULA_HOST: "nebula-cluster-graphd"
  # Port for the NebulaGraph Graph service. Default: "9669"
  NEBULA_PORT: "9669"
  # The name of the Secret containing NebulaGraph username and password (e.g., "username" and "password" keys).
  # Example: "nebula-cluster-nebula-account-root" (references your Kubeblocks credential Secret)
  NEBULA_CREDENTIALS_SECRET_NAME: ""
  # Username and Password Priority:
  # 1. If NEBULA_CREDENTIALS_SECRET_NAME is set, attempts to fetch 'username' and 'password' from that Secret.
  # 2. If NEBULA_CREDENTIALS_SECRET_NAME is empty, uses the direct values of NEBULA_USER and NEBULA_PASSWORD.
  # 3. If direct values are also empty, defaults to "root" for user or forces a failure for password.
  # ⚠️ Recommend to use NEBULA_CREDENTIALS_SECRET_NAME
  NEBULA_USER: "root" # Default NebulaGraph user
  # Password (Sensitive information, strongly recommended NOT to set directly in values.yaml):
  # Only use this if you are NOT using NEBULA_CREDENTIALS_SECRET_NAME and accept the security implications.
  # ⚠️ Strongly discourage directly setting sensitive passwords here. Use Secrets for production.
  # ⚠️ Recommend to use NEBULA_CREDENTIALS_SECRET_NAME
  NEBULA_PASSWORD: "nebula"
  # NebulaGraph connection pool settings
  NEBULA_MAX_CONNECTION_POOL_SIZE: "10"
  NEBULA_TIMEOUT: "60000"

api:
  dataPath: /data/aperag
  replicaCount: 1
  resources:
  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube. If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  # limits:
  #   cpu: 100m
  #   memory: 128Mi
  # requests:
  #   cpu: 100m
  #   memory: 128Mi
  # api must be co-located with celery-worker in order to handle uploaded documents
  affinity:
    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchLabels:
              app.aperag.io/component: celery-worker
          topologyKey: kubernetes.io/hostname
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 100
          podAffinityTerm:
            labelSelector:
              matchLabels:
                app.aperag.io/component: api
            topologyKey: kubernetes.io/hostname
  env:
    # Auth
    JWT_SECRET: "a-very-secret-key-that-you-should-change" # IMPORTANT: Change this in production
    GOOGLE_OAUTH_CLIENT_ID: ""
    GOOGLE_OAUTH_CLIENT_SECRET: ""
    GITHUB_OAUTH_CLIENT_ID: ""
    GITHUB_OAUTH_CLIENT_SECRET: ""
    AUTH_TYPE: cookie
    AUTH0_DOMAIN: ""
    AUTH0_CLIENT_ID: ""
    AUTHING_DOMAIN: ""
    AUTHING_APP_ID: ""
    LOGTO_DOMAIN: ""
    LOGTO_APP_ID: ""
    # Vector DB
    VECTOR_DB_TYPE: qdrant
    VECTOR_DB_CONTEXT: '{"url":"http://qdrant-cluster-qdrant-qdrant", "port":6333, "distance":"Cosine", "timeout": 1000}'
    # Object Store
    OBJECT_STORE_TYPE: local
    OBJECT_STORE_LOCAL_ROOT_DIR: /data/objects
    OBJECT_STORE_S3_ENDPOINT: ""
    OBJECT_STORE_S3_REGION: ""
    OBJECT_STORE_S3_ACCESS_KEY: ""
    OBJECT_STORE_S3_SECRET_KEY: ""
    OBJECT_STORE_S3_BUCKET: aperag
    OBJECT_STORE_S3_PREFIX_PATH: ""
    OBJECT_STORE_S3_USE_PATH_STYLE: "True"

    DEBUG: "False"
    DOCRAY_HOST: http://aperag-docray:8639
    MAX_BOT_COUNT: 10
    MAX_COLLECTION_COUNT: 50
    MAX_DOCUMENT_COUNT: 1000
    MAX_CONVERSATION_COUNT: 100
    EMBEDDING_MAX_CHUNKS_IN_BATCH: 10
    # Chunking
    CHUNK_SIZE: 400
    CHUNK_OVERLAP_SIZE: 20
    TIKTOKEN_CACHE_DIR: /root/.cache/tiktoken
    DEFAULT_ENCODING_MODEL: cl100k_base
    TOKENIZERS_PARALLELISM: "false"

    GRAPH_INDEX_KV_STORAGE: PGOpsSyncKVStorage
    GRAPH_INDEX_VECTOR_STORAGE: PGOpsSyncVectorStorage
    GRAPH_INDEX_GRAPH_STORAGE: PGOpsSyncGraphStorage

    # Cache
    CACHE_ENABLED: "True"
    CACHE_TTL: "86400"

    # LLM Keyword Extraction
    LLM_KEYWORD_EXTRACTION_PROVIDER: "openrouter"
    LLM_KEYWORD_EXTRACTION_MODEL: "google/gemini-2.5-flash"

  livenessProbe:
    httpGet:
      path: /health
      port: 8000
    initialDelaySeconds: 10
    periodSeconds: 10
    timeoutSeconds: 2
    failureThreshold: 3
    successThreshold: 1
  readinessProbe:
    httpGet:
      path: /health
      port: 8000
    initialDelaySeconds: 5
    periodSeconds: 5
    timeoutSeconds: 2
    failureThreshold: 3
    successThreshold: 1

celery-worker:
  replicaCount: 1
  embeddingDevice: "cpu"
  resources: { }
    # We usually recommend not to specify default resources and to leave this as a conscious
    # choice for the user. This also increases chances charts run on environments with little
    # resources, such as Minikube. If you do want to specify resources, uncomment the following
    # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
    # limits:
    #   cpu: 100m
    #   memory: 128Mi
    # requests:
    #   cpu: 100m
    #   memory: 128Mi
  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          labelSelector:
            matchLabels:
              app.aperag.io/component: celery-worker
          topologyKey: kubernetes.io/hostname
  livenessProbe:
    exec:
      command:
        - sh
        - -c
        - "celery -A config.celery status -d celery@$(hostname) > /dev/null 2>&1"
    initialDelaySeconds: 15
    periodSeconds: 30
    timeoutSeconds: 5
    failureThreshold: 3
    successThreshold: 1
  readinessProbe:
    exec:
      command:
        - sh
        - -c
        - "celery -A config.celery status -d celery@$(hostname) > /dev/null 2>&1"
    initialDelaySeconds: 15
    periodSeconds: 30
    timeoutSeconds: 5
    failureThreshold: 3
    successThreshold: 1

celerybeat:
  replicaCount: 1
  resources: { }
    # We usually recommend not to specify default resources and to leave this as a conscious
    # choice for the user. This also increases chances charts run on environments with little
    # resources, such as Minikube. If you do want to specify resources, uncomment the following
    # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
    # limits:
    #   cpu: 100m
    #   memory: 128Mi
    # requests:
    #   cpu: 100m
    #   memory: 128Mi

flower:
  replicaCount: 1
  user: admin
  password: admin
  resources: { }
    # We usually recommend not to specify default resources and to leave this as a conscious
    # choice for the user. This also increases chances charts run on environments with little
    # resources, such as Minikube. If you do want to specify resources, uncomment the following
    # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
    # limits:
    #   cpu: 100m
    #   memory: 128Mi
    # requests:
    #   cpu: 100m
    #   memory: 128Mi
  env:
    CELERY_FLOWER_USER: admin
    CELERY_FLOWER_PASSWORD: admin

# Frontend configuration
frontend:
  replicaCount: 1
  image:
    repository: "docker.io/apecloud/aperag-frontend"  # Full image name including registry
    tag: "v0.0.0-nightly"
    pullPolicy: IfNotPresent
  resources: {}
    # We usually recommend not to specify default resources and to leave this as a conscious
    # choice for the user. This also increases chances charts run on environments with little
    # resources, such as Minikube. If you do want to specify resources, uncomment the following
    # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
    # limits:
    #   cpu: 100m
    #   memory: 128Mi
    # requests:
    #   cpu: 50m
    #   memory: 64Mi
  affinity: {}
  # Frontend configuration values for settings.js
  title: "ApeRAG"
  favicon: ""
  logoDark: ""
  logoLight: ""
  github: "https://github.com/apecloud/ApeRAG"
  publicIps: []

docray:
  # Use existing service instead of deploying a new one
  # useExistingService: "http://<your-docray-service>.svc.cluster.local:8639"
  useExistingService: ""

  # Enable/disable doc-ray deployment, if useExistingService is set, this will be ignored
  enabled: true
  image:
    repository: "docker.io/apecloud/doc-ray"
    tag: "v0.2.0"
    pullPolicy: "IfNotPresent"
  service:
    type: ClusterIP

  # Default resources for CPU-only deployment
  resources:
    requests:
      cpu: "4"
      memory: "10Gi"
    limits:
      cpu: "8"
      memory: "10Gi"

  gpu:
    enabled: false
    # Resources for GPU deployment
    resources:
      requests:
        cpu: "2"
        memory: "6Gi"
        nvidia.com/gpu: "1" # Request 1 GPU
      limits:
        cpu: "8"
        memory: "10Gi"
        nvidia.com/gpu: "1" # Limit to 1 GPU

  # nodeSelector allows scheduling pods on nodes with specific labels (e.g., GPU-enabled nodes)
  # Example:
  #   accelerator: nvidia-tesla-t4
  #   cloud.google.com/gke-accelerator: nvidia-tesla-t4 # GKE example
  nodeSelector: {}
  # tolerations allow pods to be scheduled on nodes with matching taints (common for GPU nodes)
  # Example:
  # - key: "nvidia.com/gpu"
  #   operator: "Exists"
  #   effect: "NoSchedule"
  tolerations: []

  # Affinity configuration, which is more flexible than nodeSelector.
  # For example, you can use it to specify that a pod can be scheduled on one of several different GPU nodes.
  #
  # affinity:
  #   nodeAffinity:
  #     requiredDuringSchedulingIgnoredDuringExecution:
  #       nodeSelectorTerms:
  #       - matchExpressions:
  #         - key: machine.cluster.vke.volcengine.com/gpu-name # Schedule using GPU name
  #           operator: In
  #           values:
  #           - Tesla-V100 # GPU type
  #           - Tesla-T4
  affinity: {}
